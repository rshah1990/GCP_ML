{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from src.preprocessing import categorical_transform,numerical_transform,FeatureSelector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pandas_df = pd.read_csv(\"gs://custom_data/kubeflow_kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
       "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
       "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
       "       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cateforical_features = ['date', 'waterfront', 'view', 'yr_renovated']\n",
    "numerical_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "                      'condition', 'grade', 'sqft_basement', 'yr_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline(steps=[ ('cat_selector',FeatureSelector(cateforical_features)),\n",
    "                                 ('cat_transform',categorical_transform()),\n",
    "                                 ('one_hot_encoding',OneHotEncoder())\n",
    "                              ])\n",
    "\n",
    "numerical_pipeline = Pipeline(steps= [ ('num_selector', FeatureSelector(numerical_features)),\n",
    "#                                       ('num_transformer', numerical_transform()),\n",
    "                                      ('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                      ('std_scaler', StandardScaler()) \n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('cat_pipe',cat_pipeline),\n",
    "    ('num_pipe',numerical_pipeline)\n",
    "])\n",
    "\n",
    "pipeline_model = Pipeline(steps=[\n",
    "    ('full_transformation',full_pipeline),\n",
    "    ('model',RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data =pandas_df\n",
    "X = data.drop('price', axis = 1)\n",
    "#You can covert the target variable to numpy \n",
    "y = data['price'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y , test_size = 0.2 , random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'full_transformation', 'model', 'full_transformation__n_jobs', 'full_transformation__transformer_list', 'full_transformation__transformer_weights', 'full_transformation__verbose', 'full_transformation__cat_pipe', 'full_transformation__num_pipe', 'full_transformation__cat_pipe__memory', 'full_transformation__cat_pipe__steps', 'full_transformation__cat_pipe__verbose', 'full_transformation__cat_pipe__cat_selector', 'full_transformation__cat_pipe__cat_transform', 'full_transformation__cat_pipe__one_hot_encoding', 'full_transformation__cat_pipe__cat_selector__feature_names', 'full_transformation__cat_pipe__cat_transform__use_date', 'full_transformation__cat_pipe__one_hot_encoding__categories', 'full_transformation__cat_pipe__one_hot_encoding__drop', 'full_transformation__cat_pipe__one_hot_encoding__dtype', 'full_transformation__cat_pipe__one_hot_encoding__handle_unknown', 'full_transformation__cat_pipe__one_hot_encoding__sparse', 'full_transformation__num_pipe__memory', 'full_transformation__num_pipe__steps', 'full_transformation__num_pipe__verbose', 'full_transformation__num_pipe__num_selector', 'full_transformation__num_pipe__imputer', 'full_transformation__num_pipe__std_scaler', 'full_transformation__num_pipe__num_selector__feature_names', 'full_transformation__num_pipe__imputer__add_indicator', 'full_transformation__num_pipe__imputer__copy', 'full_transformation__num_pipe__imputer__fill_value', 'full_transformation__num_pipe__imputer__missing_values', 'full_transformation__num_pipe__imputer__strategy', 'full_transformation__num_pipe__imputer__verbose', 'full_transformation__num_pipe__std_scaler__copy', 'full_transformation__num_pipe__std_scaler__with_mean', 'full_transformation__num_pipe__std_scaler__with_std', 'model__bootstrap', 'model__ccp_alpha', 'model__criterion', 'model__max_depth', 'model__max_features', 'model__max_leaf_nodes', 'model__max_samples', 'model__min_impurity_decrease', 'model__min_impurity_split', 'model__min_samples_leaf', 'model__min_samples_split', 'model__min_weight_fraction_leaf', 'model__n_estimators', 'model__n_jobs', 'model__oob_score', 'model__random_state', 'model__verbose', 'model__warm_start'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('full_transformation',\n",
       "                 FeatureUnion(transformer_list=[('cat_pipe',\n",
       "                                                 Pipeline(steps=[('cat_selector',\n",
       "                                                                  FeatureSelector(feature_names=['date',\n",
       "                                                                                                 'waterfront',\n",
       "                                                                                                 'view',\n",
       "                                                                                                 'yr_renovated'])),\n",
       "                                                                 ('cat_transform',\n",
       "                                                                  categorical_transform()),\n",
       "                                                                 ('one_hot_encoding',\n",
       "                                                                  OneHotEncoder())])),\n",
       "                                                ('num_pipe',\n",
       "                                                 Pipeline(steps=[('num_selector',\n",
       "                                                                  FeatureSelector(feature_names=['bedrooms',\n",
       "                                                                                                 'bathrooms',\n",
       "                                                                                                 'sqft_living',\n",
       "                                                                                                 'sqft_lot',\n",
       "                                                                                                 'floors',\n",
       "                                                                                                 'condition',\n",
       "                                                                                                 'grade',\n",
       "                                                                                                 'sqft_basement',\n",
       "                                                                                                 'yr_built'])),\n",
       "                                                                 ('imputer',\n",
       "                                                                  SimpleImputer(strategy='median')),\n",
       "                                                                 ('std_scaler',\n",
       "                                                                  StandardScaler())]))])),\n",
       "                ('model',\n",
       "                 RandomForestRegressor(max_depth=7, n_jobs=-1,\n",
       "                                       random_state=42))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.set_params(model__random_state = 42,\n",
    "                         model__max_depth=7,\n",
    "                         model__n_jobs=-1,\n",
    "                         model__n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('full_transformation',\n",
       "                 FeatureUnion(transformer_list=[('cat_pipe',\n",
       "                                                 Pipeline(steps=[('cat_selector',\n",
       "                                                                  FeatureSelector(feature_names=['date',\n",
       "                                                                                                 'waterfront',\n",
       "                                                                                                 'view',\n",
       "                                                                                                 'yr_renovated'])),\n",
       "                                                                 ('cat_transform',\n",
       "                                                                  categorical_transform()),\n",
       "                                                                 ('one_hot_encoding',\n",
       "                                                                  OneHotEncoder())])),\n",
       "                                                ('num_pipe',\n",
       "                                                 Pipeline(steps=[('num_selector',\n",
       "                                                                  FeatureSelector(feature_names=['bedrooms',\n",
       "                                                                                                 'bathrooms',\n",
       "                                                                                                 'sqft_living',\n",
       "                                                                                                 'sqft_lot',\n",
       "                                                                                                 'floors',\n",
       "                                                                                                 'condition',\n",
       "                                                                                                 'grade',\n",
       "                                                                                                 'sqft_basement',\n",
       "                                                                                                 'yr_built'])),\n",
       "                                                                 ('imputer',\n",
       "                                                                  SimpleImputer(strategy='median')),\n",
       "                                                                 ('std_scaler',\n",
       "                                                                  StandardScaler())]))])),\n",
       "                ('model',\n",
       "                 RandomForestRegressor(max_depth=7, n_jobs=-1,\n",
       "                                       random_state=42))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([477923.25007546, 612002.71736556, 992598.90406656, ...,\n",
       "       484834.35747353, 554510.30189209, 507092.73108525])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pipeline_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222951.62471218855"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(pipeline_model.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.qwiklabs-gcp-00-a49ce03b66ad.appspot.com/\n",
      "gs://custom_data/\n",
      "gs://qwiklabs-gcp-00-a49ce03b66ad-kubeflowpipelines-default/\n",
      "gs://qwiklabs-gcp-00-a49ce03b66ad_cloudbuild/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://qwiklabs-gcp-00-a49ce03b66ad-kubeflowpipelines-default/'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "\n",
    "TRAINING_FILE_PATH=\"gs://custom_data/kubeflow_kc_house_data.csv\"\n",
    "VALIDATION_FILE_PATH=\"gs://custom_data/kubeflow_kc_house_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the hyperparameter tuning application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from src.preprocessing import categorical_transform,numerical_transform,FeatureSelector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "def train_evaluate(job_dir,dataset_path,n_estimators,max_depth, hptune):\n",
    "    # read data \n",
    "    pandas_df = pd.read_csv(\"gs://custom_data/kubeflow_kc_house_data.csv\")\n",
    "    \n",
    "    # train test split\n",
    "    data =pandas_df\n",
    "    X = data.drop('price', axis = 1)\n",
    "    y = data['price'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "    \n",
    "    cateforical_features = ['date', 'waterfront', 'view', 'yr_renovated']\n",
    "    numerical_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', \n",
    "                          'floors','condition', 'grade', 'sqft_basement', 'yr_built']\n",
    "    \n",
    "    cat_pipeline = Pipeline(steps=[ ('cat_selector',FeatureSelector(cateforical_features)),\n",
    "                                 ('cat_transform',categorical_transform()),\n",
    "                                 ('one_hot_encoding',OneHotEncoder())\n",
    "                              ])\n",
    "\n",
    "    numerical_pipeline = Pipeline(steps= [ ('num_selector', FeatureSelector(numerical_features)),\n",
    "#                                           ('num_transformer', numerical_transform()),\n",
    "                                          ('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                          ('std_scaler', StandardScaler()) \n",
    "                                         ])\n",
    "    \n",
    "    full_pipeline = FeatureUnion(transformer_list=[('cat_pipe',cat_pipeline),\n",
    "                                                    ('num_pipe',numerical_pipeline)])\n",
    "\n",
    "    pipeline_model = Pipeline(steps=[('full_transformation',full_pipeline),\n",
    "                                    ('model',RandomForestRegressor())\n",
    "                                    ])\n",
    "    pipeline_model.set_params(model__random_state = 42,\n",
    "                         model__max_depth=max_depth,\n",
    "                         model__n_jobs=-1,\n",
    "                         model__n_estimators = n_estimators)\n",
    "    \n",
    "    pipeline_model.fit(X_train, y_train)\n",
    "    rmse = np.sqrt(mean_squared_error(pipeline_model.predict(X_test),y_test))\n",
    "    \n",
    "    if hptune:\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(hyperparameter_metric_tag='rmse',\n",
    "                                                  metric_value=rmse)\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline_model, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## package script in docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "COPY src/ src/ \n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build docker image using cloud build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 5 file(s) totalling 6.4 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-gcp-00-a49ce03b66ad_cloudbuild/source/1613189903.225468-6c63c95b9a864bbab4668472149da655.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-a49ce03b66ad/locations/global/builds/63931f38-c4ed-4f2a-8da0-fb484c7474db].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/63931f38-c4ed-4f2a-8da0-fb484c7474db?project=576958157824].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"63931f38-c4ed-4f2a-8da0-fb484c7474db\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-a49ce03b66ad_cloudbuild/source/1613189903.225468-6c63c95b9a864bbab4668472149da655.tgz#1613189903601852\n",
      "Copying gs://qwiklabs-gcp-00-a49ce03b66ad_cloudbuild/source/1613189903.225468-6c63c95b9a864bbab4668472149da655.tgz#1613189903601852...\n",
      "/ [1 files][  2.5 KiB/  2.5 KiB]                                                \n",
      "Operation completed over 1 objects/2.5 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  12.29kB\n",
      "Step 1/6 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d519e2592276: Pulling fs layer\n",
      "d22d2dfcfa9c: Pulling fs layer\n",
      "b3afe92c540b: Pulling fs layer\n",
      "42499980e339: Pulling fs layer\n",
      "5cc6f3cb2c4a: Pulling fs layer\n",
      "264016c313db: Pulling fs layer\n",
      "3049a6851b27: Pulling fs layer\n",
      "f364009b5525: Pulling fs layer\n",
      "ceb8710fb121: Pulling fs layer\n",
      "60dd84bd5a31: Pulling fs layer\n",
      "a4ab234100c0: Pulling fs layer\n",
      "323ade0d04aa: Pulling fs layer\n",
      "4e0e566fd2a8: Pulling fs layer\n",
      "cc71efc47f44: Pulling fs layer\n",
      "1cb247765bd9: Pulling fs layer\n",
      "85bfe947ef8b: Pulling fs layer\n",
      "cfba0db75741: Pulling fs layer\n",
      "0803f0431169: Pulling fs layer\n",
      "60dd84bd5a31: Waiting\n",
      "a4ab234100c0: Waiting\n",
      "323ade0d04aa: Waiting\n",
      "4e0e566fd2a8: Waiting\n",
      "cc71efc47f44: Waiting\n",
      "1cb247765bd9: Waiting\n",
      "85bfe947ef8b: Waiting\n",
      "cfba0db75741: Waiting\n",
      "0803f0431169: Waiting\n",
      "42499980e339: Waiting\n",
      "5cc6f3cb2c4a: Waiting\n",
      "264016c313db: Waiting\n",
      "3049a6851b27: Waiting\n",
      "f364009b5525: Waiting\n",
      "ceb8710fb121: Waiting\n",
      "b3afe92c540b: Download complete\n",
      "d22d2dfcfa9c: Verifying Checksum\n",
      "d22d2dfcfa9c: Download complete\n",
      "42499980e339: Verifying Checksum\n",
      "42499980e339: Download complete\n",
      "d519e2592276: Verifying Checksum\n",
      "d519e2592276: Download complete\n",
      "3049a6851b27: Verifying Checksum\n",
      "3049a6851b27: Download complete\n",
      "264016c313db: Verifying Checksum\n",
      "264016c313db: Download complete\n",
      "ceb8710fb121: Verifying Checksum\n",
      "ceb8710fb121: Download complete\n",
      "60dd84bd5a31: Verifying Checksum\n",
      "60dd84bd5a31: Download complete\n",
      "a4ab234100c0: Verifying Checksum\n",
      "a4ab234100c0: Download complete\n",
      "323ade0d04aa: Verifying Checksum\n",
      "323ade0d04aa: Download complete\n",
      "f364009b5525: Verifying Checksum\n",
      "f364009b5525: Download complete\n",
      "4e0e566fd2a8: Verifying Checksum\n",
      "4e0e566fd2a8: Download complete\n",
      "cc71efc47f44: Verifying Checksum\n",
      "cc71efc47f44: Download complete\n",
      "1cb247765bd9: Verifying Checksum\n",
      "1cb247765bd9: Download complete\n",
      "85bfe947ef8b: Verifying Checksum\n",
      "85bfe947ef8b: Download complete\n",
      "0803f0431169: Verifying Checksum\n",
      "0803f0431169: Download complete\n",
      "5cc6f3cb2c4a: Verifying Checksum\n",
      "5cc6f3cb2c4a: Download complete\n",
      "d519e2592276: Pull complete\n",
      "d22d2dfcfa9c: Pull complete\n",
      "b3afe92c540b: Pull complete\n",
      "42499980e339: Pull complete\n",
      "cfba0db75741: Verifying Checksum\n",
      "cfba0db75741: Download complete\n",
      "5cc6f3cb2c4a: Pull complete\n",
      "264016c313db: Pull complete\n",
      "3049a6851b27: Pull complete\n",
      "f364009b5525: Pull complete\n",
      "ceb8710fb121: Pull complete\n",
      "60dd84bd5a31: Pull complete\n",
      "a4ab234100c0: Pull complete\n",
      "323ade0d04aa: Pull complete\n",
      "4e0e566fd2a8: Pull complete\n",
      "cc71efc47f44: Pull complete\n",
      "1cb247765bd9: Pull complete\n",
      "85bfe947ef8b: Pull complete\n",
      "cfba0db75741: Pull complete\n",
      "0803f0431169: Pull complete\n",
      "Digest: sha256:9dbaf9b5c23151fbaae3f8479c1ba2382936af933d371459c110782b86c983ad\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 86632554702c\n",
      "Step 2/6 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 26f1cd818134\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas==0.24.2) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune, fire, termcolor\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=2afdae81db638c161e91ae06f62167b958793be0d46628d92cff9b65a816684b\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=a34269abae147a670872edf376ce518f13b27ac97e68498955c03a3323b4036e\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=b49dae266768fe6e7bd0cf93dd545ba9cbeff5e56e5afe591416bb9b2d6ab20a\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built cloudml-hypertune fire termcolor\n",
      "Installing collected packages: termcolor, scikit-learn, pandas, fire, cloudml-hypertune\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.1\n",
      "    Uninstalling pandas-1.2.1:\n",
      "      Successfully uninstalled pandas-1.2.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.0 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 2.8.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 2.8.0 requires visions[type_image_path]==0.4.4, but you have visions 0.7.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 26f1cd818134\n",
      " ---> 056e67a8b232\n",
      "Step 3/6 : WORKDIR /app\n",
      " ---> Running in 33174c9f7161\n",
      "Removing intermediate container 33174c9f7161\n",
      " ---> 27692f0f118a\n",
      "Step 4/6 : COPY train.py .\n",
      " ---> c78341230af1\n",
      "Step 5/6 : COPY src/ src/\n",
      " ---> 633c48147963\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in f7560306a6f9\n",
      "Removing intermediate container f7560306a6f9\n",
      " ---> fa69828ea425\n",
      "Successfully built fa69828ea425\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-00-a49ce03b66ad/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-00-a49ce03b66ad/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-a49ce03b66ad/trainer_image]\n",
      "cb252b01e953: Preparing\n",
      "72213c5d3316: Preparing\n",
      "b41b0591f3d8: Preparing\n",
      "6f9f21c192c0: Preparing\n",
      "0f0532eed74a: Preparing\n",
      "615e303004c8: Preparing\n",
      "6ad6e8fd4ff0: Preparing\n",
      "945f0370cab4: Preparing\n",
      "289ab6c33408: Preparing\n",
      "034a4b160541: Preparing\n",
      "27b18b7fb87e: Preparing\n",
      "ae18d372a1da: Preparing\n",
      "cc450d62afb9: Preparing\n",
      "d7d0fb2f7eb0: Preparing\n",
      "3e75deadeefa: Preparing\n",
      "c77962bfc51d: Preparing\n",
      "caef3b0fe7f1: Preparing\n",
      "c39d9f02e96e: Preparing\n",
      "3a88efae17e5: Preparing\n",
      "9f10818f1f96: Preparing\n",
      "27502392e386: Preparing\n",
      "c95d2191d777: Preparing\n",
      "d7d0fb2f7eb0: Waiting\n",
      "3e75deadeefa: Waiting\n",
      "c77962bfc51d: Waiting\n",
      "caef3b0fe7f1: Waiting\n",
      "c39d9f02e96e: Waiting\n",
      "3a88efae17e5: Waiting\n",
      "9f10818f1f96: Waiting\n",
      "27502392e386: Waiting\n",
      "c95d2191d777: Waiting\n",
      "615e303004c8: Waiting\n",
      "6ad6e8fd4ff0: Waiting\n",
      "945f0370cab4: Waiting\n",
      "289ab6c33408: Waiting\n",
      "034a4b160541: Waiting\n",
      "27b18b7fb87e: Waiting\n",
      "ae18d372a1da: Waiting\n",
      "cc450d62afb9: Waiting\n",
      "0f0532eed74a: Layer already exists\n",
      "615e303004c8: Layer already exists\n",
      "6ad6e8fd4ff0: Layer already exists\n",
      "945f0370cab4: Layer already exists\n",
      "289ab6c33408: Layer already exists\n",
      "034a4b160541: Layer already exists\n",
      "27b18b7fb87e: Layer already exists\n",
      "ae18d372a1da: Layer already exists\n",
      "cc450d62afb9: Layer already exists\n",
      "d7d0fb2f7eb0: Layer already exists\n",
      "3e75deadeefa: Layer already exists\n",
      "c77962bfc51d: Layer already exists\n",
      "caef3b0fe7f1: Layer already exists\n",
      "c39d9f02e96e: Layer already exists\n",
      "3a88efae17e5: Layer already exists\n",
      "9f10818f1f96: Layer already exists\n",
      "27502392e386: Layer already exists\n",
      "c95d2191d777: Layer already exists\n",
      "72213c5d3316: Pushed\n",
      "b41b0591f3d8: Pushed\n",
      "cb252b01e953: Pushed\n",
      "6f9f21c192c0: Pushed\n",
      "latest: digest: sha256:395bfdb530cc5573d4acba5ef40f1d0b1a2a7277a9883a0893c9a209b0c966ac size: 4916\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "63931f38-c4ed-4f2a-8da0-fb484c7474db  2021-02-13T04:18:23+00:00  2M40S     gs://qwiklabs-gcp-00-a49ce03b66ad_cloudbuild/source/1613189903.225468-6c63c95b9a864bbab4668472149da655.tgz  gcr.io/qwiklabs-gcp-00-a49ce03b66ad/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on AI platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: rmse\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: n_estimators\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: max_depth\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          7,\n",
    "          9\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20210213_042105] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20210213_042105\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20210213_042105\n",
      "jobId: JOB_20210213_042105\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--dataset_path=$TRAINING_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-02-13T04:21:07Z'\n",
      "etag: xoznWDcLzAU=\n",
      "jobId: JOB_20210213_042105\n",
      "state: PREPARING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --dataset_path=gs://custom_data/kubeflow_kc_house_data.csv\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: rmse\n",
      "    maxParallelTrials: 4\n",
      "    maxTrials: 4\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 200.0\n",
      "      - 500.0\n",
      "      parameterName: n_estimators\n",
      "      type: DISCRETE\n",
      "    - discreteValues:\n",
      "      - 7.0\n",
      "      - 9.0\n",
      "      parameterName: max_depth\n",
      "      type: DISCRETE\n",
      "  jobDir: gs://qwiklabs-gcp-00-a49ce03b66ad-kubeflowpipelines-default//jobs/JOB_20210213_042105\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/qwiklabs-gcp-00-a49ce03b66ad/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20210213_042105?project=qwiklabs-gcp-00-a49ce03b66ad\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20210213_042105&project=qwiklabs-gcp-00-a49ce03b66ad\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JOB_20210213_042105'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20210213_042105',\n",
       " 'trainingInput': {'args': ['--dataset_path=gs://custom_data/kubeflow_kc_house_data.csv',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'n_estimators',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [200, 500]},\n",
       "    {'parameterName': 'max_depth',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [7, 9]}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'rmse',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://qwiklabs-gcp-00-a49ce03b66ad-kubeflowpipelines-default//jobs/JOB_20210213_042105',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/qwiklabs-gcp-00-a49ce03b66ad/trainer_image:latest'}},\n",
       " 'createTime': '2021-02-13T04:21:07Z',\n",
       " 'startTime': '2021-02-13T04:21:09Z',\n",
       " 'state': 'RUNNING',\n",
       " 'trainingOutput': {'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'rmse'},\n",
       " 'etag': 'in2TTNfKIuc='}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'trials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-7b45654dbb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainingOutput'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trials'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'trials'"
     ]
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
